{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e33591",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ale\n",
    "!pip install atari-py\n",
    "!pip install gym[atari]==0.15.7\n",
    "!pip install autorom\n",
    "!pip install autorom[accept-rom-license]\n",
    "!pip install keras\n",
    "!pip install keras-rl2\n",
    "!pip install tensorflow-gpu\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1821a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from rl.agents import DQNAgent\n",
    "#from rl.memory import SequentialMemory\n",
    "#from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "#dqn.load_weights('SavedWeights/5h-Fast/dqn_weights.h5f')\n",
    "#scores = dqn.test(env, nb_episodes=1, visualize=True)\n",
    "#print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8139e676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NON LOOPED CELL\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices()))\n",
    "\n",
    "if tf.test.gpu_device_name(): \n",
    "\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "     \n",
    "else:\n",
    "\n",
    "   print(\"Please install GPU version of TF\")\n",
    "\n",
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height, width, channels)))\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), nb_steps = 1000, attr='eps', value_max=1., value_min=.1, value_test=.2)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy\n",
    "                   ,enable_dueling_network=True\n",
    "                   ,dueling_type='avg'\n",
    "                   ,nb_actions=actions\n",
    "                   ,nb_steps_warmup=10000\n",
    "                  )\n",
    "    return dqn\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('Freeway-v0')\n",
    "height, width, channels = env.observation_space.shape\n",
    "\n",
    "actions = env.action_space.n\n",
    "model = build_model(height, width, channels, actions)\n",
    "\n",
    "weights_filename = 'dqn_{}_weights.h5f'\n",
    "checkpoint_weights_filename = 'dqn_' + 'freeway' + '_weights_{step}.h5f'\n",
    "log_filename = 'dqn_{}_log.json'\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=100)]\n",
    "callbacks += [FileLogger(log_filename, interval=10)]\n",
    "\n",
    "dqn = build_agent(model,actions)\n",
    "dqn.compile(optimizer = Adam(learning_rate=1),metrics=[\"mae\"])\n",
    "his = dqn.fit(env, nb_steps=10000, visualize=False, verbose=1)\n",
    "\n",
    "scores = dqn.test(env, nb_episodes=1, visualize=True)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#dqn.save_weights('SavedWeights/50-Fast/dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b27cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('SavedWeights/1000-10000-11000-lr1e-19Fast/dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2073096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(his.history).plot(figsize=(8, 3))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 3) # set the vertical range to [0-1]\n",
    "plt.show()\n",
    "\n",
    "plt.plot(his.history['nb_steps'], his.history['episode_reward'])\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Step')\n",
    "plt.title(\"Loss/Step\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12a020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
